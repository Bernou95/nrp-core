/*! \page python_client Running NRP-Core experiments from Python

NRP-Core experiments can be run from the provided application, \ref nrp_simulation.
In this way, the experiment is loaded from a specified configuration file and run until the specified timeout.
Additionally, we provide a Python module, *nrp_core.client.NrpCore*, which can be used to start experiments from Python and which enable more control over the experiment execution.

The main class used to launch and control NRP-Core experiment is *nrp_core.client.NrpCore*.
When instantiated, it internally launches an NRPCoreSim process in \ref server_mode "server mode" and forwards requests made through its methods to the NRPCoreSim GRPC server.
The NRPCoreSim process can be launched either as a forked process or inside of a docker container.
More details on these options are given below.

nrp_core.client.NrpCore provides methods which match the available services in NRPCoreSim server: initialize(), run_loop(int number_of_iterations), run_until_timeout(), stop(), reset() and shutdown().
Each of these methods calls the corresponding GRPC service as described \ref server_mode "here".

Below there is an example on how to use **NrpCore** for launching the \ref getting_started_experiment_tf_exchange "tf_exchange" experiment and running it:

\code{.py}
# Import NrpCore
from nrp_core.client import NrpCore
# Start NRPCoreSim server with tf_exchange experiment and connects to it
n = NrpCore('localhost:5345',"nrp-core/examples/tf_exchange","simulation_config.json")
# Initialize the experiment
n.initialize()
# Run the simulation loop for 10 iterations
n.run_loop(10)
# Check the current state of the experiment
n.current_state()
# Resets the experiment
n.reset()
# Run the experiment until timeout
n.run_until_timeout()
# Shut down the experiment
n.shutdown()
\endcode

NrpCore takes as arguments:
- address (str): the address that will be used by NRPCoreSim server
- experiment_folder (str): path to the folder containing all files needed to execute the experiment. By default is an empty string, which is interpreted as the current folder
- config_file (str): path to the experiment configuration file. It can be an absolute path or relative to `experiment_folder`. By default, "simulation_config.json"
- args (str): additional NRPCoreSim command line parameters
- log_output (bool): if true, console output from NRPCoreSim process is hidden and logged into a file .console_output.log in the experiment folder. It is true by default.
- server_timeout (int): time in seconds for waiting for connecting to the NRPCoreSim grpc server. 10 seconds by default.
- docker_daemon_address (str): IP address of the docker daemon used to launch the experiment in a docker container (more detail below). 'unix:///var/run/docker.sock' by default, which is the value expected when docker daemon is running locally with its default configuration.
- image_name (str): name of the docker image which will be used to run the experiment (more detail below). Empty by default.
- get_archives (list): list of archives (files or folders) that should be retrieved from the docker container when shutting down the simulation (eg. folder containing logged simulation data from a data transfer engine). Empty list by default.

Each of the methods provided to control the experiment returns a boolean value which is True if the request was processed correctly and False otherwise.
They can return False either because the request was not possible from the current state of the experiment (see \ref experiment_lifecycle "here" for more information on the experiment lifecycle FSM) or because there was an error while executing the request.
In the latter case the experiment is transitioned to `Failed` state and only `shutdown()` is possible.

Finally, all the request methods are blocking, i.e. the return only after the request has been processed and the corresponding GRPC service returns. 
With the exception of *run_loop()* and *run_until_timeout()*, which accepts a parameter *run_async (bool)* which allows to execute the request in a separate thread, and therefore non-blockingly.
When called with *run_async=True* they return a *Thread* object which can be used to monitor the state of the request.
In combination with the *stop()* request and the \ref datatransfer_engine "datatransfer" engine, this option could be used to monitor the simulation and stop it after some condition fulfills or visualize or process experiment data in realtime.

\section python_client_status_function Passing data between client and engines

A mechanism was implemented to allow passing data from the client to the engines, as well as the other way around.
The data for the engines must be in JSON format and can be passed as optional argument named `client_data` to the `run_loop` method of `NrpCore` Python object.
Once passed to the NRP-core process, this data becomes accessible inside the so-called \ref status_function "Status Function".
There, it can be translated into DataPacks that will then be routed to the proper engines.

The data may also be passed from the engines to the client. DataPacks coming from the engines can be accessed in the Status Function,
where they must be translated into JSON format. The JSON object can then be passed back to the client,
which will return the data (still in JSON format) as the result of `run_loop` method.

Please note that it is possible to run multiple simulation steps with a single call to `run_loop` and still use this data passing mechanism.
In that case, the Status Function will still be called on every Simulation Step, and it will reuse the same `client_data` on every call.
The data returned from the Status Function to the client will contain only the data returned from the last call.

More information about how to write the Status Function can be found on this \ref status_function "page".

\code{.py}
reference = {}
reference["velocity"] = 5
# Pass data from the client to the status function
result = n.run_loop(1, client_data=reference)
# Extract the data coming from the status function
print(result["position"])
\endcode

\section python_client_docker Launching Experiments in Docker Containers

By default, when instantiating a `NrpCore` object, NRPCoreSim is started in a forked process.
In case of willing to run NRPCoreSim in a docker container instead, set the parameters `image_name`, and optionally `docker_daemon_address`, as described above.

When running NRPCoreSim in a container from the NrpCore Python client, the behavior of the latter remains unchanged, i.e. as explained above in this page.
The only noticeable difference / constraint, is that the path specified in the `config_file` parameter of NrpCore client must be relative to `experiment_folder`, and the configuration file should be itself placed inside of the experiment folder.
This is the normal case anyways, therefore this constraint will go unnoticed most of the times.

As explained above, if `log_output` is set to True when instantiating the NrpCore client, the experiment console output is logged to a file named `.console_output.log`. When running the experiment in a docker container this file is fetched from the container and saved to `experiment_folder` when the experiment is shutdown.

In the same way, it is possible to specify in the NrpCore constructor an additional list of archives (files or folders) that should be fetched from the container and saved to `experiment_folder` when the experiment is shutdown. 
These archives are passed to the constructor using the `get_archives` parameter.

The latter is useful for example when including a \ref datatransfer_engine "Data Transfer Engine" in the experiment. 
In this case datapacks are either streamed to an MQTT topic (which can be subscribed to from outside the container) or logged into a file.
These files are stored into a folder (`data` by default) which can be added to the `get_archives` list parameter so the logs are retrieved from the container before stopping it.

<b>NOTE:</b> 

In order to launch experiments in docker containers, a docker daemon must be accessible at the IP specified in the parameter `docker_daemon_address`.
See this \ref configuring_docker_daemon "guide" for more information on how to do this.

Also keep in mind that the address of the NRPCoreSim server ('address' parameter in the NrpCore constructor) will be an address in the host where docker daemon is running, and must be accessible from the host where NrpCore is instantiated.
*/