/*! \page datapacks Engine DataPacks

DataPacks are simple objects which wrap around arbitrary data structures, like JSON objects or protobuf messages.
They provide the necessary abstract interface, which is understood by all components of NRP-Core, while still
allowing to pass data in various formats.

A DataPack consists of two parts:

- DataPack ID: which allows to uniquely indentify the object 
- DataPack data: this is the data stored by the DataPack, which can be in principle of any type

DataPacks are mainly used by \ref transceiver_function "Transceiver functions" to relay data between engines. 
Each engine type is designed to accept only datapacks of a certain type and structure.
To discover which datapack types can be processed by an engine, check out the engine's documentation \ref nrp_engines "here".


\section datapacks_id DataPack ID

Every datapack contains a DataPackIdentifier, which uniquely identifies the datapack object and allows
for routing of the data between transceiver functions, engine clients and engine servers.
A datapack identifier consists of three fields:

- name - name of the datapack. Must be unique.
- type - string representation of the DataPack data type. This field will most probably will be of no concern for the users.
It is set and used internally and is not in human-readable form.
- engine name - name of the engine to which the datapack is bound.

These fields can be accessed from the transceiver functions:

\code{.py}
print(datapack.name)
print(datapack.type)
print(datapack.engine_name)
\endcode


\section datapacks_data DataPack data

DataPack is a template class with a single template parameter, which specifies the type of the data contained by the DataPack. 
This DataPack data can be in principle of any type. 
In practice there are some limitations though, since DataPacks, which are C++ objects, must be accessible from TransceiverFunctions, which are written in Python. 
Therefore the only DataPack data types which can be actually used in nrp-core are those for which Python bindings are provided. 
These are commented \ref supported_datapack_types "below".

In TransceiverFunctions, the DataPack data can always be accessed using the datapack "data" attribute.

\section empty_datapack Empty DataPacks

It is possible for a datapack to contain no data. 
This is useful for example when an Engine is asked for a certain DataPack but there it is not able to provide it. 
In this case, an Engine can return an empty datapack.
This type of datapack contains only a datapack identifier and no data. 

Attempting to retrieve the data from an empty DataPack will result in an exception.
A method "isEmpty" is provided to check whether a DataPack is empty or not before attempting to access its data:

\code{.py}
if(not datapack.isEmpty()):
	# It's safe to get the data
	print(datapack.data)
else:
	# This will raise an exception
	print(datapack.data)
\endcode


\section datapacks_tfs Role of DataPacks in TransceiverFunctions

DataPacks are both the input and output of TransceiverFunctions.
When a datapack is declared as input of a TF, this datapack is always requested to the corresponding Engine when the latter is synchronized.
When a datapack is returned by a TF, it is sent to the corresponding Engine \ref transceiver_function_synchronization "after the TF is executed".

The subsections below elaborate on the details of to use DataPacks in TFs.

\subsection datapacks_tfs_input DataPacks as input to transceiver functions

DataPacks can be declared as TransceiverFunction inputs using the dedicated decorator.
After that they can be accessed in TFs as input arguments.

\code{.py}
# Declare datapack with "datapack_name" name from engine "engine_name" as input using the @FromEngineDataPack decorator
# The trasceiver function must accept an argument with the same name as "keyword" in the datapack decorator

@FromEngineDataPack(keyword="datapack", id=DataPackIdentifier("datapack_name", "engine_name"))
@TransceiverFunction("engine_name")
def transceiver_function(datapack):
	print(datapack.data)

# Multiple input datapacks from different engines can be declared

@FromEngineDataPack(keyword="datapack1", id=DataPackIdentifier("datapack_name1", "engine_name1"))
@FromEngineDataPack(keyword="datapack2", id=DataPackIdentifier("datapack_name2", "engine_name2"))
@TransceiverFunction("engine_name1")
def transceiver_function(datapack1, datapack2):
	print(datapack1.data)
	print(datapack2.data)
\endcode

When passed as TF arguments, DataPacks behave at all effect as read-only objects.
Even though it is possible to modify their data or to add new attributes to them inside of a TransceiverFunction,
these changes will have no effect outside of the TransceiverFunction.


\subsection datapacks_tfs_output DataPacks as output of transceiver functions

DataPacks can be returned from the transceiver function.

\code{.py}
# NRP-Core expects transceiver functions to always return a list of datapacks

@TransceiverFunction("engine_name")
def transceiver_function():
	datapack = JsonDataPack("datapack_name", "engine_name")

	return [ datapack ]

# Multiple datapacks can be returned

@TransceiverFunction("engine_name")
def transceiver_function():
	datapack1 = JsonDataPack("datapack_name1", "engine_name")
	datapack2 = JsonDataPack("datapack_name2", "engine_name")

	return [ datapack1, datapack2 ]
\endcode

\section supported_datapack_types Supported DataPack data types

As commented in the section above, DataPacks are both the input and output of TFs. Therefore, a conversion mechanism between C++ and Python is required for each supported DataPack data type. The types currently supported are nlohmann::json and protobuf messages. The subsections below give details of the Python API provided for each of these types.

\subsection datapacks_json JsonDataPack

JsonDataPack type wraps around <a href="https://github.com/nlohmann/json">nlohmann::json</a> objects.
This type of datapack is very flexible and allows to pass most types of data between the simulators and transceiver function
without writing any additional code.
The JsonDataPack can contain all basic Python types.

Inside transceiver functions the data can be accessed like a python dictionary:

\code{.py}
# To set the data

datapack = JsonDataPack("datapack_name", "engine_name")

datapack.data["null"]   = None
datapack.data["long"]   = 1
datapack.data["double"] = 43.21
datapack.data["string"] = "string"
datapack.data["bool"]   = True
datapack.data["array"]  = [5, 1, 6]
datapack.data["tuple"]  = (1, 2, 3)
datapack.data["object"] = {"key1": "value", "key2": 600}

# To retrieve the data

print(datapack.data["string"])
print(datapack.data["object"])

\endcode


\subsection datapacks_protobuf Protobuf datapacks

In contrast with JsonDataPack, which can wrap any nlohmann::json C++ object, a Python wrapper class is generated for each Protobuf definition.
For example, for the *Camera* message listed below (which is used by the \ref gazebo_engine), a python class *GazeboCameraDataPack* is generated.

\code{.proto}
package Gazebo;

// Data coming from gazebo camera datapack
message Camera
{
    uint32 imageWidth  = 1;
    uint32 imageHeight = 2;
    uint32 imageDepth  = 3;
    bytes  imageData   = 4;
}
\endcode

This class contains a *data* attribute which is of type *GazeboCamera* and gives access to the wrapped datapack data. 
The generated Python classes match the original Protobuf Python API as described in the <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated">protobuf documentation</a>.
There are some known limitations with respect to the original Protobuf Python API which are listed below with references to the protobuf documentation:

1. Well Known Types not supported <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated#wkt">ref</a>
2. Repeated Message field not supported <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated#repeated-message-fields">ref</a>
3. Map field type not supported <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated#map-fields">ref</a>
4. Only basic Enum support. To set or get *Enum* fields only *int* can be used. *Enum constants* can't be accessed from python <a href="https://developers.google.com/protocol-buffers/docs/reference/python-generated#enum">ref</a>
5. The *Message* Python wrapper only supports a subset of the methods listed <a href="https://googleapis.dev/python/protobuf/latest/google/protobuf/message.html">here</a>. These are: 'Clear', 'ClearField', 'HasField', 'IsInitialized' and 'WhichOneof'.

Finally, these Python wrappers are automatically generated in the nrp-core build process. See this \ref tutorial_add_proto_definition "guide" to know how to add custom message definitions so they become afterwards available to Engines and TFs.


\section datapacks_implementation Implementation details

All concrete datapack classes should be based on the DataPack class. It is a template class,
and the single template argument specifies the data structure type, which is going to be held by the class instances.

The DataPack class design is somewhat similar to that of std::unique_ptr. Whenever a datapack object is constructed,
it takes ownership of the input data structure. This structure may be then accessed and modified,
or the ownership may be released.

The DataPack class inherits from DataPackInterface. This class may also be instantiated, but the object will not carry any data (ie. it's an empty DataPack).

\subsection datapacks_implementation_empty Empty datapacks

A DataPack class is considered empty when its data is released. Every instance of the base class, DataPackInterface, is also
considered empty, because there is no data stored in it.

\subsection datapacks_implementation_python Python interface

In order to be accessible to transceiver functions, a conversion mechanism between C++ and Python must be specified for each DataPack data type.
Currently nrp-core provides Python bindings for nlohmann::json and protobuf messages.
In case you wished to integrate a different data type, you would have to implement Python bindings for this type and make them available to nrp-core as a Python module.

 */
